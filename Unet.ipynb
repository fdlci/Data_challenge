{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Essentials\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import os\n",
    "from tifffile import TiffFile\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "# Albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# Local \n",
    "from unet import UNet\n",
    "from LCD import LandCoverData\n",
    "from dataset import *\n",
    "from train import *\n",
    "from utils import *\n",
    "from metrics import *\n",
    "from losses import *\n",
    "LCD = LandCoverData()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏èSeed everything!\n",
    "It's important to seed everything for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìú Set all variables here\n",
    "In this cell, we define all hyperparameters that we will be using for the rest of the notebook. It helps to group them in one place so we can track them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNET_resnet18_40_epochs_0.001_learningrate_64_batchsize_seed_2021_CHANNELS_4\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'resnet18'\n",
    "SEGMENT = 'UNET'\n",
    "OPTIMIZER = 'adam'\n",
    "NB_EPOCHS = 40\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "IN_CHANNELS = 4\n",
    "\n",
    "filename = f\"{SEGMENT}_{MODEL}_{NB_EPOCHS}_epochs_{LEARNING_RATE}_learningrate_{BATCH_SIZE}_batchsize_seed_{seed}_CHANNELS_{IN_CHANNELS}\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.ToFloat(max_value=65535.0),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "    A.FromFloat(max_value=65535.0),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(1, 1, 1, 1), max_pixel_value=65535),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(1, 1, 1, 1), max_pixel_value=65535),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate datasets\n",
    "Here we perform a train/validation split in otder to evaluate our model. We then feed them to the ImageSegementationDataset so to make the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train set contains 14792 elements\nValidation set contains 3699 elements\nTest set contains 5043 elements\n"
     ]
    }
   ],
   "source": [
    "train_dir='dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "train_idx, val_idx = train_val_dataset(train_dir, val_split=0.2)\n",
    "train_set = ImageSegementationDataset(train_dir, in_channels=IN_CHANNELS, path_index=train_idx, mode='train', transforms=train_transform)\n",
    "val_set = ImageSegementationDataset(train_dir, in_channels=IN_CHANNELS, path_index=val_idx, mode='valid', transforms=test_transform)\n",
    "test_set = ImageSegementationDataset(test_dir, in_channels=IN_CHANNELS, mode='test', transforms=test_transform)\n",
    "\n",
    "print(\"Train set contains\", len(train_set), \"elements\")\n",
    "print(\"Validation set contains\", len(val_set), \"elements\")\n",
    "print(\"Test set contains\", len(test_set), \"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 232 batches in the training set\nThere are 58 batches in the validation set\n"
     ]
    }
   ],
   "source": [
    "loader_train = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_valid = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_test = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data_sizes = {\"train\": len(loader_train), \"valid\": len(loader_valid)}\n",
    "print(\"There are\", data_sizes['train'], \"batches in the training set\")\n",
    "print(\"There are\", data_sizes['valid'], \"batches in the validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing our model along with the loss and optimizers and feed that to the Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(model, segment, optimizer, nb_epochs, learning_rate, batch_size, in_channels, loss):\n",
    "    if segment == 'UNET':\n",
    "        model = smp.Unet(encoder_name=MODEL,in_channels=IN_CHANNELS, classes=10, activation=None)\n",
    "    elif segment == 'LINKNET':\n",
    "        model = smp.LinkNet(encoder_name=MODEL,in_channels=IN_CHANNELS, classes=10, activation=None)\n",
    "    if optimizer == 'adam':\n",
    "        optimizer_ft = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer_ft = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, patience=5, factor=0.5)\n",
    "    criterion = loss\n",
    "    loaders = {\n",
    "        \"train\": loader_train,\n",
    "        \"val\": loader_valid\n",
    "    }\n",
    "    return model, loaders, optimizer_ft, criterion, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loaders, optimizer_ft, criterion, scheduler = prepare(\n",
    "    model = MODEL, \n",
    "    segment = SEGMENT, \n",
    "    optimizer = OPTIMIZER, \n",
    "    nb_epochs = NB_EPOCHS, \n",
    "    learning_rate = LEARNING_RATE, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    in_channels = IN_CHANNELS,\n",
    "    loss = CombinedLoss())\n",
    "trainer = Trainer(model, loaders, optimizer_ft, criterion, scheduler, device)\n",
    "trainer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(3, reduce_on_plateau=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3ff5bc2af44638baa3b37a941d8166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is saved as test_submission.csv\n"
     ]
    }
   ],
   "source": [
    "trainer.generate_submission(loader_test, \"test_submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = trainer.get_best_model()\n",
    "torch.save(best_params,f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_saved_model(model_name):\n",
    "    \"\"\"Loads the saved model\"\"\"\n",
    "    model = unet\n",
    "    model.load_state_dict(torch.load(model_name, map_location = device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_loaded = loading_saved_model(\"unet.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ecbb290622efe6a108e88b8a5dccb3aa582d66a3ed03b3b3ca754b0c02090994"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}