{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import os\n",
    "from tifffile import TiffFile\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# Local \n",
    "from unet import UNet\n",
    "from LCD import LandCoverData\n",
    "from dataset import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCD = LandCoverData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n  (encoder1): Sequential(\n    (enc1conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc1relu1): ReLU(inplace=True)\n    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc1relu2): ReLU(inplace=True)\n  )\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder2): Sequential(\n    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc2relu1): ReLU(inplace=True)\n    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc2relu2): ReLU(inplace=True)\n  )\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder3): Sequential(\n    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc3relu1): ReLU(inplace=True)\n    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc3relu2): ReLU(inplace=True)\n  )\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder4): Sequential(\n    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc4relu1): ReLU(inplace=True)\n    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (enc4relu2): ReLU(inplace=True)\n  )\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (bottleneck): Sequential(\n    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bottleneckrelu1): ReLU(inplace=True)\n    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bottleneckrelu2): ReLU(inplace=True)\n  )\n  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n  (decoder4): Sequential(\n    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec4relu1): ReLU(inplace=True)\n    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec4relu2): ReLU(inplace=True)\n  )\n  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n  (decoder3): Sequential(\n    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec3relu1): ReLU(inplace=True)\n    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec3relu2): ReLU(inplace=True)\n  )\n  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n  (decoder2): Sequential(\n    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec2relu1): ReLU(inplace=True)\n    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec2relu2): ReLU(inplace=True)\n  )\n  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n  (decoder1): Sequential(\n    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec1relu1): ReLU(inplace=True)\n    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dec1relu2): ReLU(inplace=True)\n  )\n  (conv): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n)\n"
     ]
    }
   ],
   "source": [
    "print(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom transforms\n",
    "\n",
    "The motivation behind redefining transforms is that we need to apply the same transform to both mask and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train set contains 16 elements\nValidation set contains 4 elements\nTest set contains 34 elements\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'Small_dataset/train'\n",
    "test_dir = 'Small_dataset/test'\n",
    "\n",
    "train_set, val_set = train_val_dataset(ImageSegementationDataset(train_dir), val_split=0.2)\n",
    "test_set = ImageSegementationDataset(test_dir, mode='test')\n",
    "\n",
    "print(\"Train set contains\", len(train_set), \"elements\")\n",
    "print(\"Validation set contains\", len(val_set), \"elements\")\n",
    "print(\"Test set contains\", len(test_set), \"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train': 1, 'valid': 1}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs= 10\n",
    "lr= 0.001\n",
    "\n",
    "loader_train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_sizes = {\"train\": len(loader_train), \"valid\": len(loader_valid)}\n",
    "print(data_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "unet = UNet()\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class_weight\n",
    "weights = np.zeros((LCD.N_CLASSES,))\n",
    "num_ign_classes = len(LCD.IGNORED_CLASSES_IDX)\n",
    "weights[num_ign_classes:] = (1 / LCD.TRAIN_CLASS_COUNTS[2:])* LCD.TRAIN_CLASS_COUNTS[2:].sum() / (LCD.N_CLASSES-2)\n",
    "weights[LCD.IGNORED_CLASSES_IDX] = 0.\n",
    "\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "\n",
    "# Criterion\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = 1.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size(), \"Both vectors must have the same shape\"\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "            y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, valid_loader, data_sizes, epochs, optimizer, scheduler, title):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    num_workers = 1\n",
    "\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
    "    print(data_sizes)\n",
    "    step = 0\n",
    "\n",
    "    #class_weights = class_weight().to(device)\n",
    "    #criterion = nn.CrossEntropyLoss(class_weights)\n",
    "    criterion = dice_coef_multilabel\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_iou = 0\n",
    "            running_kl_div = 0\n",
    "\n",
    "            for image, mask in tqdm(loaders[phase]):\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                    output = model(image)\n",
    "                    # print(output)\n",
    "                    _, preds = torch.max(output, 1)\n",
    "\n",
    "                    #loss = criterion(output, torch.tensor(mask, dtype=torch.long, device=device).squeeze())\n",
    "                    # print(preds.size())\n",
    "                    # print(torch.argmax(output, axis=1).shape)\n",
    "                    loss = criterion(output, mask.squeeze(), LCD.N_CLASSES)\n",
    "                \n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # running_corrects += torch.sum(iou_pytorch(output, mask))\n",
    "                running_iou += mIOU(mask, preds)\n",
    "                running_kl_div += epsilon_kl_divergence(mask.cpu(), preds.cpu())\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss/data_sizes[phase]\n",
    "            epoch_iou = running_iou/data_sizes[phase]\n",
    "            epoch_kl = running_kl_div/data_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                training_loss.append(epoch_loss)\n",
    "            else:\n",
    "                validation_loss.append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} IoU: {:.4f} KL_div: {:.4f}'.format(phase, epoch_loss, epoch_iou, epoch_kl))\n",
    "            \n",
    "            if phase == 'valid' and epoch_iou > best_acc:\n",
    "                best_acc = epoch_iou\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            \n",
    "    # Plotting the validation loss and training loss\n",
    "    print('validation loss: ' + str(validation_loss))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # plot the training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss, 'b', label='Training Loss')\n",
    "    plt.plot(validation_loss, 'r', label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show() #Change title for every model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, model, epochs):\n",
    "    \n",
    "    class_weights = torch.FloatTensor(weights)\n",
    "\n",
    "    # Optimizing all parameters\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    #optimizer_ft = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Training the model\n",
    "    title = 'Variations of the training and validation loss'\n",
    "    model_ft = training(model, loader_train, loader_valid, data_sizes, epochs, optimizer_ft, exp_lr_scheduler, title)\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_kl_divergence(y_true, y_pred):\n",
    "    class_distribution_true = np.apply_along_axis(np.bincount, axis=1, arr=y_true.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    class_distribution_pred = np.apply_along_axis(np.bincount, axis=1, arr=y_pred.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    # Normalize to sum to 1  \n",
    "    normalized_class_distribution_true = (class_distribution_true.T/class_distribution_true.sum(1)).T\n",
    "    normalized_class_distribution_pred = (class_distribution_pred.T/class_distribution_pred.sum(1)).T\n",
    "    # add a small constant for smoothness around 0\n",
    "    normalized_class_distribution_true += 1e-7\n",
    "    normalized_class_distribution_pred += 1e-7\n",
    "\n",
    "    score = np.mean(np.sum(normalized_class_distribution_true * np.log(normalized_class_distribution_true / normalized_class_distribution_pred), 1))\n",
    "    try:\n",
    "        assert np.isfinite(score)\n",
    "    except AssertionError as e:\n",
    "        raise ValueError('score is NaN or infinite') from e\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = (y_pred_f * y_true_f).sum()\n",
    "    smooth = 0.0001\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "def change_shape(y_true, y_pred, numLabels):  \n",
    "\n",
    "    encoded_target = y_pred.data.clone().zero_()\n",
    "    encoded_target[...] = 0\n",
    "    print(torch.tensor(y_true.unsqueeze(1), dtype=torch.int64))\n",
    "    encoded_target.scatter_(1, torch.tensor(y_true.unsqueeze(1), dtype=torch.int64), 1.)\n",
    "    encoded_target = Variable(encoded_target)\n",
    "\n",
    "    return encoded_target\n",
    "\n",
    "def dice_coef_multilabel(y_pred, y_true, numLabels):\n",
    "    dice=0\n",
    "    y_true = change_shape(y_true, y_pred, numLabels)\n",
    "    # print(y_true.size())\n",
    "    # print(y_pred.size())\n",
    "    for index in range(numLabels):\n",
    "        dice += dice_coef(y_true[:,index,:,:], y_pred[:,index,:,:])\n",
    "    return dice/numLabels # taking average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, mask = next(iter(loader_train))\n",
    "random_mask = 2*torch.rand(size=(16,10,256,256))-1\n",
    "random_mask = torch.sigmoid(random_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_model('dataset/train', unet, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "unet_pre_trained = smp.Unet(encoder_name='resnet18',in_channels=4, classes=10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device cpu\n{'train': 1, 'valid': 1}\nEpoch 1/10\n----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45bc98b0dcd846a49f5a2b77303b62fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([12, 256, 256])\ntensor([[[4, 3, 3,  ..., 4, 4, 4],\n         [4, 8, 8,  ..., 9, 9, 4],\n         [4, 8, 8,  ..., 8, 4, 4],\n         ...,\n         [4, 8, 1,  ..., 8, 1, 1],\n         [4, 3, 4,  ..., 8, 3, 3],\n         [6, 8, 8,  ..., 4, 8, 3]],\n\n        [[4, 4, 1,  ..., 4, 4, 4],\n         [4, 8, 7,  ..., 4, 3, 4],\n         [4, 8, 3,  ..., 7, 7, 4],\n         ...,\n         [4, 3, 8,  ..., 8, 1, 1],\n         [4, 4, 4,  ..., 4, 4, 1],\n         [6, 8, 7,  ..., 7, 8, 8]],\n\n        [[4, 3, 4,  ..., 3, 8, 4],\n         [4, 7, 8,  ..., 3, 3, 4],\n         [4, 8, 4,  ..., 3, 8, 1],\n         ...,\n         [3, 8, 8,  ..., 8, 8, 3],\n         [4, 4, 4,  ..., 1, 8, 3],\n         [6, 8, 6,  ..., 8, 8, 8]],\n\n        ...,\n\n        [[4, 4, 1,  ..., 3, 3, 4],\n         [4, 8, 8,  ..., 3, 4, 4],\n         [4, 8, 8,  ..., 3, 4, 4],\n         ...,\n         [4, 8, 8,  ..., 0, 8, 9],\n         [4, 4, 4,  ..., 7, 4, 8],\n         [6, 8, 8,  ..., 3, 8, 6]],\n\n        [[0, 4, 4,  ..., 4, 4, 4],\n         [4, 8, 8,  ..., 3, 3, 4],\n         [4, 8, 4,  ..., 3, 3, 4],\n         ...,\n         [4, 8, 8,  ..., 8, 1, 3],\n         [4, 3, 4,  ..., 4, 1, 4],\n         [7, 6, 6,  ..., 8, 8, 8]],\n\n        [[4, 4, 4,  ..., 3, 4, 4],\n         [4, 4, 3,  ..., 1, 3, 4],\n         [4, 8, 4,  ..., 3, 7, 4],\n         ...,\n         [4, 8, 8,  ..., 4, 1, 3],\n         [4, 4, 4,  ..., 4, 3, 4],\n         [6, 8, 6,  ..., 8, 8, 8]]])\ntensor([[[4, 4, 4,  ..., 6, 6, 6],\n         [4, 4, 4,  ..., 6, 6, 6],\n         [4, 4, 4,  ..., 6, 6, 6],\n         ...,\n         [3, 3, 3,  ..., 6, 6, 6],\n         [3, 3, 3,  ..., 6, 6, 6],\n         [3, 3, 3,  ..., 6, 6, 6]],\n\n        [[3, 3, 3,  ..., 3, 3, 3],\n         [3, 3, 3,  ..., 3, 3, 3],\n         [3, 3, 3,  ..., 3, 3, 3],\n         ...,\n         [3, 3, 3,  ..., 3, 3, 3],\n         [2, 2, 3,  ..., 3, 3, 3],\n         [3, 3, 3,  ..., 3, 3, 3]],\n\n        [[3, 3, 3,  ..., 4, 4, 4],\n         [3, 3, 3,  ..., 4, 4, 4],\n         [3, 3, 3,  ..., 4, 4, 4],\n         ...,\n         [4, 4, 4,  ..., 6, 6, 6],\n         [4, 4, 4,  ..., 6, 6, 6],\n         [4, 4, 4,  ..., 6, 6, 6]],\n\n        ...,\n\n        [[3, 6, 6,  ..., 3, 3, 3],\n         [3, 6, 6,  ..., 3, 3, 3],\n         [3, 6, 6,  ..., 3, 3, 3],\n         ...,\n         [6, 6, 6,  ..., 3, 3, 3],\n         [6, 6, 6,  ..., 3, 3, 3],\n         [6, 6, 6,  ..., 3, 3, 3]],\n\n        [[3, 3, 3,  ..., 6, 6, 6],\n         [3, 3, 3,  ..., 6, 6, 6],\n         [3, 3, 3,  ..., 6, 6, 6],\n         ...,\n         [6, 6, 6,  ..., 6, 6, 6],\n         [6, 6, 6,  ..., 6, 6, 6],\n         [6, 3, 3,  ..., 6, 6, 6]],\n\n        [[3, 3, 3,  ..., 4, 4, 4],\n         [3, 3, 3,  ..., 4, 4, 4],\n         [3, 3, 3,  ..., 4, 4, 4],\n         ...,\n         [3, 3, 6,  ..., 4, 4, 4],\n         [3, 3, 6,  ..., 4, 4, 4],\n         [6, 6, 6,  ..., 4, 4, 4]]], dtype=torch.uint8)\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3d9f92184de9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset/train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munet_pre_trained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-c03dc68deec4>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data_dir, model, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Variations of the training and validation loss'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel_ft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-1b0fe20ea7df>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, train_loader, valid_loader, data_sizes, epochs, optimizer, scheduler, title)\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;31m#loss = criterion(output, torch.tensor(mask, dtype=torch.long, device=device).squeeze())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-bfd23004e99d>\u001b[0m in \u001b[0;36mdice_coef_multilabel\u001b[1;34m(y_true, y_pred, numLabels)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdice\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdice_coef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdice\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnumLabels\u001b[0m \u001b[1;31m# taking average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "train_model('dataset/train', unet_pre_trained, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet_pre_trained.state_dict(), \"unet-resnet18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_unet = train_model('dataset/train', unet_pre_trained, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_pre_trained_bis = smp.Unet(encoder_name='vgg11',in_channels=4, classes=10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_unet_bis = train_model('dataset/train', unet_pre_trained_bis, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_saved_model(model_name):\n",
    "    \"\"\"Loads the saved model\"\"\"\n",
    "    model = unet\n",
    "    model.load_state_dict(torch.load(model_name, map_location = device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_loaded = loading_saved_model(\"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['no_data', 'clouds', 'artificial', 'cultivated', 'broadleaf', 'coniferous', 'herbaceous', 'natural', 'snow', 'water']\n",
    "\n",
    "ids_test = np.arange(10)\n",
    "\n",
    "def compute_class_counts(masks, n_classes):\n",
    "\n",
    "    dist = np.zeros((masks.size(0), n_classes))\n",
    "\n",
    "    for i, mask in enumerate(masks):\n",
    "        arr = mask.numpy()\n",
    "        count = np.bincount(arr.ravel(), minlength=n_classes)\n",
    "        count[0], count[1] = 0, 0\n",
    "        dist[i] = count/np.sum(count)\n",
    "    return dist\n",
    "\n",
    "def getting_pixel_distribution(model, test_loader, test_size):\n",
    "\n",
    "    indx_test = np.arange(10087,10087+test_size, dtype=np.int32).reshape(-1,1)\n",
    "    all_dist = np.zeros((test_size, LCD.N_CLASSES))\n",
    "\n",
    "    for counter, images in enumerate(tqdm(test_loader)):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        distributions = compute_class_counts(preds, LCD.N_CLASSES)\n",
    "        all_dist[counter*images.size(0):(counter+1)*images.size(0)] = distributions\n",
    "\n",
    "    df = pd.DataFrame(all_dist, columns=column_names)\n",
    "    df.insert(0, 'Sample_id', range(10087, 10087 + len(df)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'dataset/test'\n",
    "test_set = ImageSegementationDataset(test_dir, mode='test')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "test_size = len(test_set)\n",
    "\n",
    "df = getting_pixel_distribution(model_loaded, test_loader, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_from_pandas_csv(df):\n",
    "    df.to_csv(r'results.csv', index = False)\n",
    "\n",
    "create_from_pandas_csv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ecbb290622efe6a108e88b8a5dccb3aa582d66a3ed03b3b3ca754b0c02090994"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}