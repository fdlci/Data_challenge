{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Essentials\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import os\n",
    "from tifffile import TiffFile\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "# Albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# Local \n",
    "from unet import UNet\n",
    "from LCD import LandCoverData\n",
    "from dataset import *\n",
    "from train import *\n",
    "\n",
    "LCD = LandCoverData()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏èSeed everything!\n",
    "It's important to seed everything for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìú Set all variables here\n",
    "In this cell, we define all hyperparameters that we will be using for the rest of the notebook. It helps to group them in one place so we can track them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'resnet18'\n",
    "OPTIMIZER = 'adam'\n",
    "NB_EPOCHS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "IN_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resnet18_20_epochs_0.01_learningrate_8_batchsize_seed_2021'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{MODEL}_{NB_EPOCHS}_epochs_{LEARNING_RATE}_learningrate_{BATCH_SIZE}_batchsize_seed_{seed}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.ToFloat(max_value=65535.0),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "    A.FromFloat(max_value=65535.0),\n",
    "    A.Normalize(mean=(0, 0, 0, 0), std=(1, 1, 1, 1), max_pixel_value=65535),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(mean=(0, 0, 0, 0), std=(1, 1, 1, 1), max_pixel_value=65535),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate datasets\n",
    "Here we perform a train/validation split in otder to evaluate our model. We then feed them to the ImageSegementationDataset so to make the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set contains 14792 elements\n",
      "Validation set contains 3699 elements\n",
      "Test set contains 5043 elements\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "\n",
    "train_idx, val_idx = train_val_dataset(train_dir, val_split=0.2)\n",
    "train_set = ImageSegementationDataset(train_dir, in_channels=IN_CHANNELS, path_index=train_idx, mode='train', transforms=train_transform)\n",
    "val_set = ImageSegementationDataset(train_dir, in_channels=IN_CHANNELS, path_index=val_idx, mode='valid', transforms=test_transform)\n",
    "test_set = ImageSegementationDataset(test_dir, in_channels=IN_CHANNELS, mode='test', transforms=test_transform)\n",
    "\n",
    "print(\"Train set contains\", len(train_set), \"elements\")\n",
    "print(\"Validation set contains\", len(val_set), \"elements\")\n",
    "print(\"Test set contains\", len(test_set), \"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1849 batches in the training set\n",
      "There are 463 batches in the validation set\n"
     ]
    }
   ],
   "source": [
    "loader_train = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_valid = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_test = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data_sizes = {\"train\": len(loader_train), \"valid\": len(loader_valid)}\n",
    "print(\"There are\", data_sizes['train'], \"batches in the training set\")\n",
    "print(\"There are\", data_sizes['valid'], \"batches in the validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = (y_pred_f * y_true_f).sum()\n",
    "    smooth = 0.0001\n",
    "    return 1 - (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "def change_shape(y_true, y_pred, numLabels):  \n",
    "\n",
    "    encoded_target = y_pred.data.clone().zero_()\n",
    "    encoded_target[...] = 0\n",
    "    encoded_target.scatter_(1, torch.tensor(y_true.unsqueeze(1), dtype=torch.int64), 1.)\n",
    "    encoded_target = Variable(encoded_target)\n",
    "    return encoded_target\n",
    "\n",
    "def dice_coef_multilabel(y_pred, y_true, numLabels):\n",
    "    dice=0\n",
    "    y_true = change_shape(y_true, y_pred, numLabels)\n",
    "    for index in range(numLabels):\n",
    "        dice += dice_coef(y_true[:,index,:,:], y_pred[:,index,:,:])\n",
    "    return dice/numLabels # taking average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIOU(label, pred, num_classes=10):\n",
    "    iou_list = list()\n",
    "    present_iou_list = list()\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    # Note: Following for loop goes from 0 to (num_classes-1)\n",
    "    # and ignore_index is num_classes, thus ignore_index is\n",
    "    # not considered in computation of IoU.\n",
    "    for sem_class in range(num_classes):\n",
    "        pred_inds = (pred == sem_class)\n",
    "        target_inds = (label == sem_class)\n",
    "        if target_inds.long().sum().item() == 0:\n",
    "            iou_now = float('nan')\n",
    "        else: \n",
    "            intersection_now = (pred_inds[target_inds]).long().sum().item()\n",
    "            union_now = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection_now\n",
    "            iou_now = float(intersection_now) / float(union_now)\n",
    "            present_iou_list.append(iou_now)\n",
    "        iou_list.append(iou_now)\n",
    "    return np.mean(present_iou_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_kl_divergence(y_true, y_pred):\n",
    "    class_distribution_true = np.apply_along_axis(np.bincount, axis=1, arr=y_true.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    class_distribution_pred = np.apply_along_axis(np.bincount, axis=1, arr=y_pred.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    # Normalize to sum to 1  \n",
    "    normalized_class_distribution_true = (class_distribution_true.T/class_distribution_true.sum(1)).T\n",
    "    normalized_class_distribution_pred = (class_distribution_pred.T/class_distribution_pred.sum(1)).T\n",
    "    # add a small constant for smoothness around 0\n",
    "    normalized_class_distribution_true += 1e-7\n",
    "    normalized_class_distribution_pred += 1e-7\n",
    "\n",
    "    score = np.mean(np.sum(normalized_class_distribution_true * np.log(normalized_class_distribution_true / normalized_class_distribution_pred), 1))\n",
    "    try:\n",
    "        assert np.isfinite(score)\n",
    "    except AssertionError as e:\n",
    "        raise ValueError('score is NaN or infinite') from e\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, valid_loader, data_sizes, epochs, optimizer, scheduler, title):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    num_workers = 1\n",
    "\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 1000\n",
    "\n",
    "    loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
    "    step = 0\n",
    "\n",
    "    class_weights = class_weight().to(device)\n",
    "    criterion_1 = nn.CrossEntropyLoss(class_weights)\n",
    "    criterion_2 = dice_coef_multilabel\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_iou = 0\n",
    "            running_kl_div = 0\n",
    "\n",
    "            for image, mask in tqdm(loaders[phase]):\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                \n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                    output = model(image)\n",
    "                    _, preds = torch.max(output, 1)\n",
    "\n",
    "                    #loss = criterion(output, torch.tensor(mask, dtype=torch.long, device=device).squeeze())\n",
    "                    loss = 0.75*criterion_1(output, torch.tensor(mask, dtype=torch.long, device=device).squeeze())+criterion_2(output, mask.squeeze(), LCD.N_CLASSES)\n",
    "                \n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_iou += mIOU(mask, preds)\n",
    "                running_kl_div += epsilon_kl_divergence(mask.cpu(), preds.cpu())\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss/data_sizes[phase]\n",
    "            epoch_iou = running_iou/data_sizes[phase]\n",
    "            epoch_kl = running_kl_div/data_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                training_loss.append(epoch_loss)\n",
    "            else:\n",
    "                validation_loss.append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} IoU: {:.4f} KL_div: {:.4f}'.format(phase, epoch_loss, epoch_iou, epoch_kl))\n",
    "            \n",
    "            if phase == 'valid' and epoch_kl < best_acc:\n",
    "                best_acc = epoch_kl\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(),\"unet_timm-efficientnet-b3_3_channels.pt\")\n",
    "\n",
    "            \n",
    "    # Plotting the validation loss and training loss\n",
    "    print('validation loss: ' + str(validation_loss))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # plot the training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss, 'b', label='Training Loss')\n",
    "    plt.plot(validation_loss, 'r', label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show() #Change title for every model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, model, epochs, learning_rate):\n",
    "    # Optimizing all parameters\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Training the model\n",
    "    title = 'Variations of the training and validation loss SGD'\n",
    "    model_ft = training(model, loader_train, loader_valid, data_sizes, epochs, optimizer_ft, exp_lr_scheduler, title)\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_pre_trained = smp.Unet(encoder_name='resnet18',in_channels=IN_CHANNELS, classes=10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda:0\n",
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27b1677b984451c9a4405479fccb703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-bc74afc39743>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = 0.75*criterion_1(output, torch.tensor(mask, dtype=torch.long, device=device).squeeze())+criterion_2(output, mask.squeeze(), LCD.N_CLASSES)\n",
      "<ipython-input-55-78a706cb2fa7>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_target.scatter_(1, torch.tensor(y_true.unsqueeze(1), dtype=torch.int64), 1.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.0650 IoU: 0.3571 KL_div: 0.4930\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e7ddac36874d1fa5d4d4886be114d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.9574 IoU: 0.4079 KL_div: 0.3755\n",
      "Epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2a929c483340c6b0cca8f0622095fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.9156 IoU: 0.4432 KL_div: 0.1900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc332a70c9e45ebbd92b1de92a9497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.8621 IoU: 0.4902 KL_div: 0.1609\n",
      "Epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d592ce2be6204212b05a4878a7c13e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8667 IoU: 0.4775 KL_div: 0.1266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e84a61fac5841089ef59d4cf6ea744b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.9251 IoU: 0.4375 KL_div: 0.2018\n",
      "Epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e3e549b30d4ed0a785d4a6475a5a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8484 IoU: 0.4904 KL_div: 0.1077\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb957dcbab44ed88367ac5cd5fbb684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.8099 IoU: 0.5251 KL_div: 0.0898\n",
      "Epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f710d5fcdd443fb6731db419d2fec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8332 IoU: 0.5003 KL_div: 0.0975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5122f9eec2074686819a8c4a4e57d2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.8024 IoU: 0.5281 KL_div: 0.0781\n",
      "Epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0152352026474777b11f465f7fc707f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8288 IoU: 0.5040 KL_div: 0.0923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a897354d5620442a8b884ff8f28a52ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.8786 IoU: 0.4738 KL_div: 0.2930\n",
      "Epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5683a550e8964cfd9a4475bc237af278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8182 IoU: 0.5105 KL_div: 0.0830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aca81ddf664810998ff350132e5a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.7906 IoU: 0.5368 KL_div: 0.0851\n",
      "Epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0f1f36f4384d339660b124afe75627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7951 IoU: 0.5286 KL_div: 0.0676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7a2ad9460d4f06b2e8452058faa616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.7692 IoU: 0.5470 KL_div: 0.0668\n",
      "Epoch 9/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151d989ea8fd449aa6db5bd4462b3b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7919 IoU: 0.5314 KL_div: 0.0634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e863cee238491faf1ade693c5b8ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.7695 IoU: 0.5481 KL_div: 0.0729\n",
      "Epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b32bcfcaa7349569d913289e5c0b90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7901 IoU: 0.5338 KL_div: 0.0629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9032d616ab247c78a533b34adb2fd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.7668 IoU: 0.5500 KL_div: 0.0651\n",
      "Epoch 11/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f066eb64d7cc44c380fd03cd6fcca7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7879 IoU: 0.5347 KL_div: 0.0610\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba59232736194d61a57dd63ef9143c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.7690 IoU: 0.5533 KL_div: 0.0627\n",
      "Epoch 12/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f7221a10144e22b9c13d233e479050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unet_pre_trained = train_model('dataset/train', unet_pre_trained, epochs=NB_EPOCHS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94bde4829634bc9bcba726bd4781f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "running_iou = 0\n",
    "running_kl_div = 0\n",
    "unet_pre_trained.eval()\n",
    "for image, mask in tqdm(loader_valid):\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output = unet_pre_trained(image)\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        running_iou += mIOU(mask, preds)\n",
    "        running_kl_div += epsilon_kl_divergence(mask.cpu(), preds.cpu())\n",
    "\n",
    "\n",
    "epoch_iou = running_iou/len(loader_valid)\n",
    "epoch_kl = running_kl_div/len(loader_valid)\n",
    "\n",
    "print('Validation IoU', epoch_iou, \"Validation KL divergence\", epoch_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet_pre_trained.state_dict(),\"unet_timm-efficientnet-b3_62_epochs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_distribution(y):\n",
    "    class_distribution = np.apply_along_axis(np.bincount, axis=1, arr=y.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    # Normalize to sum to 1  \n",
    "    return (class_distribution.T/class_distribution.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83da850d3e46475f94aa6e92ff802d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_dict = {\"sample_id\":[], \"no_data\":[],\"clouds\":[],\"artificial\":[],\"cultivated\":[],\"broadleaf\":[],\"coniferous\":[],\"herbaceous\":[],\"natural\":[],\"snow\":[],\"water\":[]}\n",
    "for image, path in tqdm(loader_test):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = unet_pre_trained(image)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        class_dis = batch_distribution(preds.cpu())\n",
    "        sub_dict[\"sample_id\"] += [int(p.split('.')[0]) for p in path]\n",
    "        for key in LCD.CLASSES:\n",
    "            sub_dict[key] += class_dis[:,LCD.CLASSES.index(key)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>no_data</th>\n",
       "      <th>clouds</th>\n",
       "      <th>artificial</th>\n",
       "      <th>cultivated</th>\n",
       "      <th>broadleaf</th>\n",
       "      <th>coniferous</th>\n",
       "      <th>herbaceous</th>\n",
       "      <th>natural</th>\n",
       "      <th>snow</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>10087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>0.248520</td>\n",
       "      <td>0.196350</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.528641</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>10088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>0.410492</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.405426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>10089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>0.259369</td>\n",
       "      <td>0.226807</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>0.426941</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>10090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>0.517822</td>\n",
       "      <td>0.148590</td>\n",
       "      <td>0.285034</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>10091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026031</td>\n",
       "      <td>0.363190</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.397568</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id  no_data  clouds  artificial  cultivated  broadleaf  \\\n",
       "3436      10087      0.0     0.0    0.007431    0.248520   0.196350   \n",
       "3073      10088      0.0     0.0    0.004990    0.410492   0.176392   \n",
       "497       10089      0.0     0.0    0.055969    0.259369   0.226807   \n",
       "1319      10090      0.0     0.0    0.009201    0.037643   0.517822   \n",
       "2484      10091      0.0     0.0    0.026031    0.363190   0.188400   \n",
       "\n",
       "      coniferous  herbaceous   natural  snow     water  \n",
       "3436    0.013412    0.528641  0.000122   0.0  0.005524  \n",
       "3073    0.002701    0.405426  0.000000   0.0  0.000000  \n",
       "497     0.024384    0.426941  0.000015   0.0  0.006516  \n",
       "1319    0.148590    0.285034  0.001709   0.0  0.000000  \n",
       "2484    0.024353    0.397568  0.000229   0.0  0.000229  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.DataFrame.from_dict(sub_dict)\n",
    "df_sub = df_sub.sort_values(by='sample_id')\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_saved_model(model_name):\n",
    "    \"\"\"Loads the saved model\"\"\"\n",
    "    model = unet\n",
    "    model.load_state_dict(torch.load(model_name, map_location = device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_loaded = loading_saved_model(\"unet.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2s",
   "language": "python",
   "name": "l2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
