{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Essentials\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import os\n",
    "from tifffile import TiffFile\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "# Albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# Local \n",
    "from unet import UNet\n",
    "from LCD import LandCoverData\n",
    "from dataset import *\n",
    "from train import *\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom transforms\n",
    "\n",
    "The motivation behind redefining transforms is that we need to apply the same transform to both mask and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCD = LandCoverData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.ToFloat(max_value=65535.0),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.Flip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.25),\n",
    "    A.IAAEmboss(p=0.25),\n",
    "    A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "    A.FromFloat(max_value=65535.0),\n",
    "    A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=65535),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=65535),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set contains 14792 elements\n",
      "Validation set contains 3699 elements\n",
      "Test set contains 5043 elements\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "\n",
    "train_idx, val_idx = train_val_dataset(train_dir, val_split=0.2)\n",
    "train_set = ImageSegementationDataset(train_dir, in_channels=3, path_index=train_idx, mode='train', transforms=train_transform)\n",
    "val_set = ImageSegementationDataset(train_dir, in_channels=3, path_index=val_idx, mode='valid', transforms=test_transform)\n",
    "test_set = ImageSegementationDataset(test_dir, in_channels=3, mode='test', transforms=test_transform)\n",
    "\n",
    "print(\"Train set contains\", len(train_set), \"elements\")\n",
    "print(\"Validation set contains\", len(val_set), \"elements\")\n",
    "print(\"Test set contains\", len(test_set), \"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1849, 'valid': 463}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs= 10\n",
    "lr= 0.001\n",
    "\n",
    "loader_train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_sizes = {\"train\": len(loader_train), \"valid\": len(loader_valid)}\n",
    "print(data_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = (y_pred_f * y_true_f).sum()\n",
    "    smooth = 0.0001\n",
    "    return 1 - (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "def change_shape(y_true, y_pred, numLabels):  \n",
    "\n",
    "    encoded_target = y_pred.data.clone().zero_()\n",
    "    encoded_target[...] = 0\n",
    "    encoded_target.scatter_(1, torch.tensor(y_true.unsqueeze(1), dtype=torch.int64), 1.)\n",
    "    encoded_target = Variable(encoded_target)\n",
    "    return encoded_target\n",
    "\n",
    "def dice_coef_multilabel(y_pred, y_true, numLabels):\n",
    "    dice=0\n",
    "    y_true = change_shape(y_true, y_pred, numLabels)\n",
    "    for index in range(numLabels):\n",
    "        dice += dice_coef(y_true[:,index,:,:], y_pred[:,index,:,:])\n",
    "    return dice/numLabels # taking average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIOU(label, pred, num_classes=10):\n",
    "    iou_list = list()\n",
    "    present_iou_list = list()\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    # Note: Following for loop goes from 0 to (num_classes-1)\n",
    "    # and ignore_index is num_classes, thus ignore_index is\n",
    "    # not considered in computation of IoU.\n",
    "    for sem_class in range(num_classes):\n",
    "        pred_inds = (pred == sem_class)\n",
    "        target_inds = (label == sem_class)\n",
    "        if target_inds.long().sum().item() == 0:\n",
    "            iou_now = float('nan')\n",
    "        else: \n",
    "            intersection_now = (pred_inds[target_inds]).long().sum().item()\n",
    "            union_now = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection_now\n",
    "            iou_now = float(intersection_now) / float(union_now)\n",
    "            present_iou_list.append(iou_now)\n",
    "        iou_list.append(iou_now)\n",
    "    return np.mean(present_iou_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_kl_divergence(y_true, y_pred):\n",
    "    class_distribution_true = np.apply_along_axis(np.bincount, axis=1, arr=y_true.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    class_distribution_pred = np.apply_along_axis(np.bincount, axis=1, arr=y_pred.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    # Normalize to sum to 1  \n",
    "    normalized_class_distribution_true = (class_distribution_true.T/class_distribution_true.sum(1)).T\n",
    "    normalized_class_distribution_pred = (class_distribution_pred.T/class_distribution_pred.sum(1)).T\n",
    "    # add a small constant for smoothness around 0\n",
    "    normalized_class_distribution_true += 1e-7\n",
    "    normalized_class_distribution_pred += 1e-7\n",
    "\n",
    "    score = np.mean(np.sum(normalized_class_distribution_true * np.log(normalized_class_distribution_true / normalized_class_distribution_pred), 1))\n",
    "    try:\n",
    "        assert np.isfinite(score)\n",
    "    except AssertionError as e:\n",
    "        raise ValueError('score is NaN or infinite') from e\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weight():\n",
    "    weights = np.zeros((LCD.N_CLASSES,))\n",
    "    num_ign_classes = len(LCD.IGNORED_CLASSES_IDX)\n",
    "    weights[num_ign_classes:] = (1 / LCD.TRAIN_CLASS_COUNTS[2:])* LCD.TRAIN_CLASS_COUNTS[2:].sum() / (LCD.N_CLASSES-2)\n",
    "    weights[LCD.IGNORED_CLASSES_IDX] = 0.\n",
    "\n",
    "    class_weights = torch.FloatTensor(weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, valid_loader, data_sizes, epochs, optimizer, scheduler, title):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    num_workers = 1\n",
    "\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 1000\n",
    "\n",
    "    loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
    "    print(data_sizes)\n",
    "    step = 0\n",
    "\n",
    "    #class_weights = class_weight().to(device)\n",
    "    #criterion = nn.CrossEntropyLoss(class_weights)\n",
    "    criterion = dice_coef_multilabel\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_iou = 0\n",
    "            running_kl_div = 0\n",
    "\n",
    "            for image, mask in tqdm(loaders[phase]):\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                    output = model(image)\n",
    "                    # print(output)\n",
    "                    _, preds = torch.max(output, 1)\n",
    "\n",
    "                    #loss = criterion(output, torch.tensor(mask, dtype=torch.long, device=device).squeeze())\n",
    "                    # print(preds.size())\n",
    "                    # print(torch.argmax(output, axis=1).shape)\n",
    "                    loss = criterion(output, mask.squeeze(), LCD.N_CLASSES)\n",
    "                \n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # running_corrects += torch.sum(iou_pytorch(output, mask))\n",
    "                running_iou += mIOU(mask, preds)\n",
    "                running_kl_div += epsilon_kl_divergence(mask.cpu(), preds.cpu())\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss/data_sizes[phase]\n",
    "            epoch_iou = running_iou/data_sizes[phase]\n",
    "            epoch_kl = running_kl_div/data_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                training_loss.append(epoch_loss)\n",
    "            else:\n",
    "                validation_loss.append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} IoU: {:.4f} KL_div: {:.4f}'.format(phase, epoch_loss, epoch_iou, epoch_kl))\n",
    "            \n",
    "            if phase == 'valid' and epoch_kl < best_acc:\n",
    "                best_acc = epoch_kl\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(),\"unet_timm-efficientnet-b3_3_channels.pt\")\n",
    "\n",
    "            \n",
    "    # Plotting the validation loss and training loss\n",
    "    print('validation loss: ' + str(validation_loss))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # plot the training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss, 'b', label='Training Loss')\n",
    "    plt.plot(validation_loss, 'r', label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show() #Change title for every model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, model, epochs):\n",
    "    # Optimizing all parameters\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Training the model\n",
    "    title = 'Variations of the training and validation loss SGD'\n",
    "    model_ft = training(model, loader_train, loader_valid, data_sizes, epochs, optimizer_ft, exp_lr_scheduler, title)\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_pre_trained = smp.Unet(encoder_name='resnet18',in_channels=3, classes=10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda:0\n",
      "{'train': 1849, 'valid': 463}\n",
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac44851889934aefa41e5c97e59a87c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-78a706cb2fa7>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_target.scatter_(1, torch.tensor(y_true.unsqueeze(1), dtype=torch.int64), 1.)\n"
     ]
    }
   ],
   "source": [
    "unet_pre_trained = train_model('dataset/train', unet_pre_trained, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94bde4829634bc9bcba726bd4781f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "running_iou = 0\n",
    "running_kl_div = 0\n",
    "unet_pre_trained.eval()\n",
    "for image, mask in tqdm(loader_valid):\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output = unet_pre_trained(image)\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        running_iou += mIOU(mask, preds)\n",
    "        running_kl_div += epsilon_kl_divergence(mask.cpu(), preds.cpu())\n",
    "\n",
    "\n",
    "epoch_iou = running_iou/len(loader_valid)\n",
    "epoch_kl = running_kl_div/len(loader_valid)\n",
    "\n",
    "print('Validation IoU', epoch_iou, \"Validation KL divergence\", epoch_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet_pre_trained.state_dict(),\"unet_timm-efficientnet-b3_62_epochs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_distribution(y):\n",
    "    class_distribution = np.apply_along_axis(np.bincount, axis=1, arr=y.flatten(1), minlength=LCD.N_CLASSES)\n",
    "    # Normalize to sum to 1  \n",
    "    return (class_distribution.T/class_distribution.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no_data',\n",
       " 'clouds',\n",
       " 'artificial',\n",
       " 'cultivated',\n",
       " 'broadleaf',\n",
       " 'coniferous',\n",
       " 'herbaceous',\n",
       " 'natural',\n",
       " 'snow',\n",
       " 'water']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCD.CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83da850d3e46475f94aa6e92ff802d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_dict = {\"sample_id\":[], \"no_data\":[],\"clouds\":[],\"artificial\":[],\"cultivated\":[],\"broadleaf\":[],\"coniferous\":[],\"herbaceous\":[],\"natural\":[],\"snow\":[],\"water\":[]}\n",
    "for image, path in tqdm(loader_test):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = unet_pre_trained(image)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        class_dis = batch_distribution(preds.cpu())\n",
    "        sub_dict[\"sample_id\"] += [int(p.split('.')[0]) for p in path]\n",
    "        for key in LCD.CLASSES:\n",
    "            sub_dict[key] += class_dis[:,LCD.CLASSES.index(key)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>no_data</th>\n",
       "      <th>clouds</th>\n",
       "      <th>artificial</th>\n",
       "      <th>cultivated</th>\n",
       "      <th>broadleaf</th>\n",
       "      <th>coniferous</th>\n",
       "      <th>herbaceous</th>\n",
       "      <th>natural</th>\n",
       "      <th>snow</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>10087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>0.248520</td>\n",
       "      <td>0.196350</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.528641</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>10088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>0.410492</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.405426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>10089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>0.259369</td>\n",
       "      <td>0.226807</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>0.426941</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>10090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>0.517822</td>\n",
       "      <td>0.148590</td>\n",
       "      <td>0.285034</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>10091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026031</td>\n",
       "      <td>0.363190</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.397568</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id  no_data  clouds  artificial  cultivated  broadleaf  \\\n",
       "3436      10087      0.0     0.0    0.007431    0.248520   0.196350   \n",
       "3073      10088      0.0     0.0    0.004990    0.410492   0.176392   \n",
       "497       10089      0.0     0.0    0.055969    0.259369   0.226807   \n",
       "1319      10090      0.0     0.0    0.009201    0.037643   0.517822   \n",
       "2484      10091      0.0     0.0    0.026031    0.363190   0.188400   \n",
       "\n",
       "      coniferous  herbaceous   natural  snow     water  \n",
       "3436    0.013412    0.528641  0.000122   0.0  0.005524  \n",
       "3073    0.002701    0.405426  0.000000   0.0  0.000000  \n",
       "497     0.024384    0.426941  0.000015   0.0  0.006516  \n",
       "1319    0.148590    0.285034  0.001709   0.0  0.000000  \n",
       "2484    0.024353    0.397568  0.000229   0.0  0.000229  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.DataFrame.from_dict(sub_dict)\n",
    "df_sub = df_sub.sort_values(by='sample_id')\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_saved_model(model_name):\n",
    "    \"\"\"Loads the saved model\"\"\"\n",
    "    model = unet\n",
    "    model.load_state_dict(torch.load(model_name, map_location = device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_loaded = loading_saved_model(\"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_ids(test_images):\n",
    "    id_list = []\n",
    "    for test_image in os.listdir(os.path.join(test_images,'images')):\n",
    "        id_im = int(re.findall(r'\\d+', test_image)[0])\n",
    "        id_list.append(id_im)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['no_data', 'clouds', 'artificial', 'cultivated', 'broadleaf', 'coniferous', 'herbaceous', 'natural', 'snow', 'water']\n",
    "\n",
    "ids_test = np.arange(10)\n",
    "\n",
    "def compute_class_counts(masks, n_classes):\n",
    "\n",
    "    dist = np.zeros((masks.size(0), n_classes))\n",
    "\n",
    "    for i, mask in enumerate(masks):\n",
    "        arr = mask.cpu().numpy()\n",
    "        count = np.bincount(arr.ravel(), minlength=n_classes)\n",
    "        count[0], count[1] = 0, 0\n",
    "        dist[i] = count/np.sum(count)\n",
    "    return dist\n",
    "\n",
    "def getting_pixel_distribution(model, test_loader, test_size, ids):\n",
    "\n",
    "    indx_test = np.arange(10087,10087+test_size, dtype=np.int32).reshape(-1,1)\n",
    "    all_dist = np.zeros((test_size, LCD.N_CLASSES))\n",
    "    previous = 0\n",
    "\n",
    "    for counter, images in enumerate(tqdm(test_loader)):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        distributions = compute_class_counts(preds, LCD.N_CLASSES)\n",
    "        all_dist[previous:previous+images.size(0)] = distributions\n",
    "        previous = previous+images.size(0)\n",
    "\n",
    "    df = pd.DataFrame(all_dist, columns=column_names)\n",
    "    df.insert(0, 'Sample_id', ids)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'dataset/test'\n",
    "test_set = ImageSegementationDataset(test_dir, mode='test')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "test_size = len(test_set)\n",
    "ids = sorted(get_ids(test_dir))\n",
    "\n",
    "model_loaded.eval()\n",
    "df = getting_pixel_distribution(model_loaded, test_loader, test_size, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_from_pandas_csv(df):\n",
    "    df.to_csv(r'results.csv', index = False)\n",
    "\n",
    "create_from_pandas_csv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2s",
   "language": "python",
   "name": "l2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
